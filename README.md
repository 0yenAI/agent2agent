# Ollama A2A App

🤖 **ハイブリッドAIエージェント対話デスクトップアプリケーション**

Ollama A2A Appは、OllamaによるローカルLLMと、Gemini・Claude APIを組み合わせ、複数のAIエージェント間での対話を実現するデスクトップアプリケーションです。分析役と評価役の2つのエージェントが議論を交わすことで、単一のAIでは得られない多角的な視点や、より深い洞察を引き出すことを目的としています。

## ✨ 主な特徴

- **🔄 ハイブリッドAI対話**: Ollamaのローカルモデルと、Gemini・ClaudeのクラウドAPIを自由に組み合わせて対話させることができます。
- **🖥️ 直感的なGUI**: Python標準のtkinterライブラリで構築された、シンプルで軽量なインターフェース。
- **📝 Markdownエクスポート**: 対話の履歴を整理されたMarkdown形式で簡単に保存できます。
- **⚙️ 柔軟な設定**: 対話に使用するモデル、議論のラウンド数、各応答のタイムアウト時間をGUIから柔軟に設定できます。
- **🌍 クロスプラットフォーム**: macOSとLinuxで動作します。
- **🚀 簡単セットアップ**: 最新のPythonパッケージ管理ツール`uv`を使用し、コマンド一つで環境構築と起動が完了します。

## 🛠️ システム要件

- **OS**: macOS または Linux
- **Python**: 3.12 以降
- **uv**: [インストール](https://astral.sh/uv/install.sh)されていること
- **Ollama**: 最新版がインストールされ、サービスが起動していること

## 🚀 セットアップ & 実行

### 1. Ollamaの準備

まず、Ollamaを[公式サイト](https://ollama.com/)からインストールし、サービスを起動します。

```bash
# Ollamaサービスの起動 (ターミナルで実行)
ollama serve
```

次に対話で使用したいモデルをダウンロードします。

```bash
# 例: 軽量モデルと高性能モデルをダウンロード
ollama pull llama3:8b
ollama pull gemma:7b
```

### 2. アプリケーションのセットアップと起動

リポジトリをクローンまたはダウンロードしたディレクトリで、以下のコマンドを実行します。

```bash
# 1. 仮想環境の作成
uv venv

# 2. プロジェクトのインストール (依存関係も同時にインストールされます)
uv pip install -e .

# 3. アプリケーションの起動
uv run start
```

`uv run start` コマンドは、`pyproject.toml` に定義された `start` スクリプトを実行します。

### 3. APIキーの設定 (任意)

GeminiまたはClaudeモデルを使用する場合は、APIキーの設定が必要です。

1.  アプリケーション右上の `⚙️ 設定` ボタンをクリックします。
2.  表示されたダイアログにGeminiまたはClaudeのAPIキーを入力し、「保存」ボタンを押します。
3.  APIキーはプロジェクトルートに `.gemini_api_key` または `.claude_api_key` というファイル名で安全に保存されます。

## 📄 ライセンス

このプロジェクトは **Apache License 2.0** の下で公開されています。
